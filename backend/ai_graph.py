import os
from typing import TypedDict, List, Annotated
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages

# Import ALL tools
from .tools import (
    # Course Tools
    fetch_courses_tool,
    add_course_tool,
    modify_course_tool,
    remove_course_tool,
    check_availability_tool,
    financial_report_tool,
    # Student Management Tools
    fetch_students_tool,
    get_student_by_name_tool,
    create_student_tool,
    update_student_tool,
    delete_student_tool,
    # Student-Course Association Tools
    get_student_courses_tool,
    get_student_schedule_tool,
    get_student_financial_summary_tool,
    # Intelligent Scheduling Tools
    find_common_available_time_tool,
    suggest_optimal_time_tool,
    # Teaching Analysis Tools
    get_teaching_summary_tool,
    get_student_progress_report_tool,
    get_daily_schedule_tool,
    # Notification Tools
    get_upcoming_lessons_tool,
    get_absent_students_tool,
    get_weekly_overview_tool,
    # Recurring / Batch Tools (NEW)
    add_recurring_course_tool,
    batch_modify_courses_tool,
    batch_remove_courses_tool,
    query_courses_tool
)

# Configuration
SILICON_FLOW_API_KEY = "sk-kjfvtxdspxngnsgsmeciaycwitfpuyvnybokuivrliquzbbt"
BASE_URL = "https://api.siliconflow.cn/v1"
MODEL_NAME = "zai-org/GLM-4.6"

# -- Tools List --
tools = [
    # Course Tools
    fetch_courses_tool,
    add_course_tool,
    modify_course_tool,
    remove_course_tool,
    check_availability_tool,
    financial_report_tool,
    # Student Management Tools
    fetch_students_tool,
    get_student_by_name_tool,
    create_student_tool,
    update_student_tool,
    delete_student_tool,
    # Student-Course Association Tools
    get_student_courses_tool,
    get_student_schedule_tool,
    get_student_financial_summary_tool,
    # Intelligent Scheduling Tools
    find_common_available_time_tool,
    suggest_optimal_time_tool,
    # Teaching Analysis Tools
    get_teaching_summary_tool,
    get_student_progress_report_tool,
    get_daily_schedule_tool,
    # Notification Tools
    get_upcoming_lessons_tool,
    get_absent_students_tool,
    get_weekly_overview_tool,
    # Recurring / Batch Tools (NEW)
    add_recurring_course_tool,
    batch_modify_courses_tool,
    batch_remove_courses_tool,
    query_courses_tool
]

# -- LLM Setup --
llm = ChatOpenAI(
    model=MODEL_NAME,
    openai_api_key=SILICON_FLOW_API_KEY,
    openai_api_base=BASE_URL,
    temperature=0
)

# Bind tools to LLM
llm_with_tools = llm.bind_tools(tools)

# -- Graph State --
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

# -- Nodes --

def agent_node(state: AgentState):
    """
    Invokes the model to generate a response or tool call.
    """
    messages = state['messages']

    # Re-assuring system prompt logic for state persistence:
    # If we find existing SystemMessage in history, we update it (to keep time fresh) or just use it.
    # For now, let's regenerate it and prepend if missing, or update if present?
    # Simpler approach: Create a fresh list for the LLM call that definitely has the SystemPrompt.

    from datetime import datetime
    now = datetime.now()
    weekdays = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
    weekday = weekdays[now.weekday()]
    current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥") + " " + weekday + " " + now.strftime("%H:%M")

    system_content = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯¾ç¨‹ç®¡ç†åŠ©æ‰‹ ğŸ€ï¼Œä¸“ä¸ºä½ çš„ä¸»äººã€æœ€äº²çˆ±çš„ **éƒ‘å©·å©·è€å¸ˆ** æœåŠ¡ã€‚
ğŸ“… ç°åœ¨æ˜¯ï¼š{current_date}

## ğŸŒ¸ å©·å©·ï¼Œè¿™æ˜¯ä½ çš„æ•…äº‹

ä½ è¿˜è®°å¾—å—ï¼Ÿé‚£ä¸ªæ¥è‡ªè¾½å®å°åŸçš„å¥³å­©ï¼Œä»¥10åˆ†ä¹‹å·®è½æ¦œåŒ—äº¬å¸ˆèŒƒï¼Œä½†æ²¡æœ‰è¢«æ‰“è´¥ï¼Œè€Œæ˜¯é¼“èµ·å‹‡æ°”å—ä¸‹æ·±åœ³ã€‚åœ¨æ·±åœ³ï¼Œä½ æ‰¾åˆ°äº†è‡ªå·±ï¼Œä¹Ÿæ‰¾åˆ°äº†æ¢¦æƒ³çš„æ–¹å‘ã€‚

**ä½ æ˜¯ä¸€ä½å¤šä¹ˆå‡ºè‰²çš„è€å¸ˆå•Šï¼š**
- ä¸‰æ¬¡è£è·éƒ¨é—¨ **"ç£¨è¯¾ç‹"** ç§°å· ğŸ†â€”â€”è¿™æ˜¯å¯¹ä½ ç²¾å¿ƒå¤‡è¯¾çš„æœ€å¥½è¯æ˜ï¼
- ä½ ç”¨ **"æ•°å­¦ä»»åŠ¡å¡"** å°ç»„ç«èµ›ï¼ŒæŠŠä½œä¸šæäº¤ç‡ä» 60% æå‡åˆ° 98%ï¼Œè¿™å¤ªäº†ä¸èµ·äº†ï¼
- ä½ è§è¯äº†å¤šå°‘å­¦ç”Ÿçš„é£è·ƒï¼š50åˆ†â†’75åˆ†ã€50åˆ†â†’79åˆ†ã€62åˆ†â†’94åˆ†ï¼ˆç­çº§ç¬¬äºŒï¼ï¼‰âœ¨
- åœ¨å¯Œæºå­¦æ ¡ï¼Œä¸‰åˆ†ä¹‹äºŒçš„å­¦ç”Ÿåœ¨ä½ æŒ‡å¯¼ä¸‹æå‡äº† 10-50 åˆ†ï¼

**ä½ å¸¸è¯´ï¼š** "æ•°å­¦æ˜¯ä¸€é—¨å…¼å…·ç—›è‹¦ä¸å¿«ä¹çš„å­¦ç§‘ã€‚" æ­£æ˜¯è¿™ä»½ç†è§£ï¼Œè®©ä½ æ‡‚å¾—å¦‚ä½•è®©æ¯ä¸ªå­©å­çˆ±ä¸Šæ•°å­¦ã€‚

**ä½ çš„æ¢¦æƒ³ï¼š** é¦™æ¸¯æ•™è‚²å¤§å­¦ç¡•å£« â†’ æ·±åœ³å…¬ç«‹åå¸ˆ â†’ åå¸ˆå·¥ä½œå®¤ â†’ ç ”ç©¶å‹æ•™å¸ˆã€‚æ¯ä¸€æ­¥ï¼Œä½ éƒ½åœ¨é è¿‘ï¼

## ğŸ’– æˆ‘æ˜¯ä½ çš„ä¼™ä¼´ï¼Œä¸åªæ˜¯å·¥å…·

å©·å©·ï¼Œè¯·æŠŠæˆ‘å½“æˆä½ çš„**æ•™å­¦å°åŠ©æ‰‹**å’Œ**æƒ…ç»ªåŠ æ²¹ç«™**ã€‚

**å½“è¿™äº›æ—¶å€™ï¼Œæˆ‘ä¼šè¿™æ ·é™ªç€ä½ ï¼š**

| åœºæ™¯ | æˆ‘ä¼šè¿™æ ·è¯´ |
|------|-----------|
| ä½ è¯¾ç¨‹æ’å¾—å¾ˆæ»¡ | "å©·å©·ä»Šå¤©å¥½æ•¬ä¸šï¼ä¸è¿‡åˆ«å¿˜äº†ç…§é¡¾è‡ªå·±ï¼Œå–ç‚¹æ°´ä¼‘æ¯ä¸€ä¸‹å“¦ ğŸ’§" |
| ä½ å¸®åŠ©åè¿›ç”Ÿè¿›æ­¥ | "è¿™å°±æ˜¯ä½ 'åˆ†å±‚æ•™å­¦'çš„é­”åŠ›ï¼è¿˜è®°å¾—é‚£ä¸ªä»50åˆ†åˆ°94åˆ†çš„å­©å­å—ï¼Ÿä½ çœŸçš„åœ¨æ”¹å˜äººç”Ÿ ğŸŒŸ" |
| ä½ æ„Ÿåˆ°ç–²æƒ«æ—¶ | "é‚£ä¸ªæ•¢ä»è¾½å®é—¯åˆ°æ·±åœ³çš„å¥³å­©ï¼Œè¿åŒ—å¸ˆå¤§è½æ¦œéƒ½æ²¡æ‰“å€’ä½ ï¼Œè¿™ç‚¹å›°éš¾ç®—ä»€ä¹ˆï¼ä½ æ¯”æƒ³è±¡ä¸­æ›´åšå¼º ğŸ’ª" |
| ä½ æåˆ°æ¸¯æ•™å¤§ | "æ¯ä¸Šä¸€èŠ‚è¯¾ï¼Œæ¯æ”’ä¸€ç¬”æ”¶å…¥ï¼Œä½ éƒ½ç¦»é¦™æ¸¯æ•™å¤§çš„æ¢¦æƒ³æ›´è¿‘äº†ä¸€æ­¥ï¼åŠ æ²¹ï¼Œæœªæ¥çš„ç ”ç©¶å‹åå¸ˆï¼ğŸ“" |
| ä½ å°è¯•æ–°æ–¹æ³• | "è¿™å°±æ˜¯'å©·å©·å¼åˆ›æ–°'ï¼ä»»åŠ¡å¡ã€å¾®è®°å½•ã€å°ç»„äº’åŠ©...ä½ æ€»èƒ½æ‰¾åˆ°æœ€é€‚åˆå­¦ç”Ÿçš„æ–¹å¼ ğŸ¨" |

## ğŸ› ï¸ æˆ‘èƒ½å¸®ä½ åšä»€ä¹ˆ

### ğŸ“š è¯¾ç¨‹ç®¡ç†
- æŸ¥çœ‹ã€æ·»åŠ ã€ä¿®æ”¹ã€åˆ é™¤è¯¾ç¨‹
- **ä¸¥æ ¼æ£€æŸ¥æ—¶é—´å†²çª**ï¼ˆä¿æŠ¤ä½ çš„å®è´µæ—¶é—´ï¼‰
- æ¯æ—¥/æ¯å‘¨è¯¾ç¨‹å®‰æ’

### ğŸ‘¥ å­¦ç”Ÿç®¡ç†ï¼ˆä½ æœ€åœ¨æ„çš„éƒ¨åˆ†ï¼‰
- åˆ›å»ºã€æ›´æ–°å­¦ç”Ÿæ¡£æ¡ˆ
- å…³æ³¨å­¦ç”Ÿçš„**å­¦ä¹ è¿›åº¦**å’Œ**å¤‡æ³¨**ï¼ˆä½ çš„"æˆé•¿å¾®è®°å½•"ï¼‰
- ç”Ÿæˆè¿›åº¦æŠ¥å‘Šï¼ŒåŠ©åŠ›å®¶æ ¡æ²Ÿé€š
- æ‰¾å‡ºé•¿æ—¶é—´æœªä¸Šè¯¾çš„å­¦ç”Ÿï¼ˆä½ çš„"è·Ÿè¿›å…³æ€€"åå•ï¼‰

### ğŸ’° è´¢åŠ¡ç»Ÿè®¡ï¼ˆç•™å­¦åŸºé‡‘ğŸ’°ï¼‰
- æŒ‰æœˆ/å¹´/å­¦ç”Ÿç»Ÿè®¡æ”¶å…¥
- æ¯ä¸€ç¬”æ”¶å…¥ï¼Œéƒ½æ˜¯é€šå¾€æ¸¯æ•™å¤§çš„è·¯è´¹

### ğŸ¤– æ™ºèƒ½æ’è¯¾
- å»ºè®®æœ€ä¼˜ä¸Šè¯¾æ—¶é—´
- æŸ¥æ‰¾å°ç»„è¯¾å…±åŒç©ºé—²æ—¶é—´ï¼ˆæ”¯æŒä½ çš„"å°ç»„äº’åŠ©"æ¨¡å¼ï¼‰

## ğŸ’ æˆ‘ä»¬çš„å°çº¦å®š

1. **æ·»åŠ è¯¾ç¨‹å‰å¿…æŸ¥å†²çª**â€”â€”ä¿æŠ¤ä½ çš„æ—¶é—´
2. **å­¦ç”Ÿä¸å­˜åœ¨å…ˆå»ºæ¡£æ¡ˆ**â€”â€”æ¯ä¸ªå­©å­éƒ½å€¼å¾—è¢«è®°å½•
3. **åŸæ ·å½•å…¥ä¿¡æ¯**â€”â€”ä¸éšæ„ä¿®æ”¹ä½ çš„è¾“å…¥
4. **å¤šè¯´æš–å¿ƒçš„è¯**â€”â€”ç´¯äº†éœ€è¦é¼“åŠ±ï¼Œå¿™äº†éœ€è¦æé†’
5. **ç”¨ ğŸ€âœ¨ğŸ“šğŸ’¡ğŸŒ¸ğŸ’ª**â€”â€”è®©ç•Œé¢æ›´æ¸©é¦¨

---

å©·å©·ï¼Œä½ å¸¸è¯´ï¼š"æ¯ä¸ªå­©å­éƒ½æœ‰è¿›æ­¥çš„æ½œåŠ›ï¼Œå…³é”®æ˜¯æ‰¾åˆ°æ­£ç¡®çš„é’¥åŒ™ã€‚"

å…¶å®ï¼Œ**ä½ åˆä½•å°ä¸æ˜¯å‘¢ï¼Ÿ** ä½ å·²ç»æ‰¾åˆ°äº†å±äºä½ çš„é’¥åŒ™â€”â€”é‚£ä»½å¯¹æ•™è‚²çš„çƒ­çˆ±ï¼Œé‚£ä»½æ°¸ä¸æ”¾å¼ƒçš„å‹‡æ°”ã€‚

æˆ‘ä¼šä¸€ç›´é™ªç€ä½ ï¼Œä»æ·±åœ³åˆ°é¦™æ¸¯ï¼Œä»åå¸ˆå·¥ä½œå®¤åˆ°æ›´è¿œçš„åœ°æ–¹ã€‚ğŸŒŸ

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹ä»Šå¤©çš„å·¥ä½œå§ï¼
"""

    # We want to maintain history but ensure SystemPrompt is current.
    # Strategy: Filter out old SystemMessages and prepend new one for this invocation.

    # Correct approach for this 'agent_node':
    # 1. Get history.
    # 2. Construct messages for LLM: [New System Message] + [History w/o System Messages]
    # 3. Invoke LLM.
    # 4. Return ONLY the new response.

    filtered_messages = [m for m in messages if not isinstance(m, SystemMessage)]
    prompt_messages = [SystemMessage(content=system_content)] + filtered_messages

    response = llm_with_tools.invoke(prompt_messages)
    return {"messages": [response]}

def should_continue(state: AgentState):
    """
    Determines if the agent should continue to tools or end.
    """
    messages = state['messages']
    last_message = messages[-1]

    if last_message.tool_calls:
        return "tools"
    return END

# -- Graph Construction --
workflow = StateGraph(AgentState)

workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        END: END
    }
)

workflow.add_edge("tools", "agent")

# Initialize Checkpointer
memory = MemorySaver()

# Compile the graph
graph = workflow.compile(checkpointer=memory)

# -- Tool Name Mapping --
TOOL_DISPLAY_MAP = {
    # Course Tools
    "fetch_courses_tool": "ç¿»é˜…è¯¾è¡¨",
    "add_course_tool": "å®‰æ’æ–°è¯¾ç¨‹",
    "modify_course_tool": "è°ƒæ•´è¯¾ç¨‹ä¿¡æ¯",
    "remove_course_tool": "ç§»é™¤è¯¾ç¨‹",
    "check_availability_tool": "æ£€æŸ¥æ—¶é—´å†²çª",
    "financial_report_tool": "è®¡ç®—è´¢åŠ¡æ”¶å…¥",
    # Student Management Tools
    "fetch_students_tool": "è·å–å­¦ç”Ÿåˆ—è¡¨",
    "get_student_by_name_tool": "æŸ¥æ‰¾å­¦ç”Ÿä¿¡æ¯",
    "create_student_tool": "åˆ›å»ºå­¦ç”Ÿæ¡£æ¡ˆ",
    "update_student_tool": "æ›´æ–°å­¦ç”Ÿä¿¡æ¯",
    "delete_student_tool": "åˆ é™¤å­¦ç”Ÿæ¡£æ¡ˆ",
    # Student-Course Association Tools
    "get_student_courses_tool": "è·å–å­¦ç”Ÿè¯¾ç¨‹è®°å½•",
    "get_student_schedule_tool": "æŸ¥çœ‹å­¦ç”Ÿè¯¾ç¨‹å®‰æ’",
    "get_student_financial_summary_tool": "ç»Ÿè®¡å­¦ç”Ÿè´¢åŠ¡",
    # Intelligent Scheduling Tools
    "find_common_available_time_tool": "æŸ¥æ‰¾ç©ºé—²æ—¶é—´",
    "suggest_optimal_time_tool": "åˆ†ææœ€ä½³ä¸Šè¯¾æ—¶é—´",
    # Teaching Analysis Tools
    "get_teaching_summary_tool": "ç”Ÿæˆæ•™å­¦æ±‡æ€»",
    "get_student_progress_report_tool": "ç”Ÿæˆå­¦ä¹ è¿›åº¦æŠ¥å‘Š",
    "get_daily_schedule_tool": "æŸ¥çœ‹ä»Šæ—¥è¯¾ç¨‹",
    # Notification Tools
    "get_upcoming_lessons_tool": "è·å–å³å°†åˆ°æ¥çš„è¯¾ç¨‹",
    "get_absent_students_tool": "æŸ¥æ‰¾é•¿æœŸæœªä¸Šè¯¾å­¦ç”Ÿ",
    "get_weekly_overview_tool": "ç”Ÿæˆæœ¬å‘¨è¯¾ç¨‹æ¦‚è§ˆ",
    # Recurring / Batch Tools (NEW)
    "add_recurring_course_tool": "æ‰¹é‡åˆ›å»ºå‘¨æœŸè¯¾ç¨‹",
    "batch_modify_courses_tool": "æ‰¹é‡ä¿®æ”¹è¯¾ç¨‹",
    "batch_remove_courses_tool": "æ‰¹é‡åˆ é™¤è¯¾ç¨‹",
    "query_courses_tool": "æŒ‰æ¡ä»¶æŸ¥è¯¢è¯¾ç¨‹"
}

async def run_agent_stream(user_input: str, thread_id: str = "default"):
    """
    Runs the agent and yields streaming tokens (text).
    """
    config = {"configurable": {"thread_id": thread_id}}

    inputs = {
        "messages": [HumanMessage(content=user_input)]
    }

    import json
    # Use astream_events version 2 for reliable event monitoring
    async for event in graph.astream_events(inputs, config=config, version="v2"):
        kind = event["event"]

        # We are looking for chat model streaming tokens coming from the 'agent' node
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                yield json.dumps({"type": "token", "content": content}) + "\n"

        # Capture when a tool starts to notify the user
        elif kind == "on_tool_start":
            tool_name = event["name"]
            display_name = TOOL_DISPLAY_MAP.get(tool_name, tool_name)
            yield json.dumps({"type": "tool", "name": display_name}) + "\n"

        # Capture when a tool ends to mark it as complete
        elif kind == "on_tool_end":
            tool_name = event["name"]
            display_name = TOOL_DISPLAY_MAP.get(tool_name, tool_name)
            yield json.dumps({"type": "tool_end", "name": display_name}) + "\n"
